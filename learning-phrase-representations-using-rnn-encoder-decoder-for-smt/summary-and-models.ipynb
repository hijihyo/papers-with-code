{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"summary-and-models.ipynb","provenance":[],"collapsed_sections":["X8CHRNUh7DvB","l64McN027GDd"],"authorship_tag":"ABX9TyMtx1FrfiJJpIu1gw9+PQR4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation [1]\n","\n","## Part 1. 논문 정리와 모델 구현"],"metadata":{"id":"Co0iMgnI58Do"}},{"cell_type":"markdown","source":["# Summary"],"metadata":{"id":"NBZyW0Hq7Bgd"}},{"cell_type":"markdown","source":["다양한 분야에서 Deep neural networks (DNNs) 를 도입하려는 움직임이 나타나면서, 구 단위 SMT 시스템 (phrase-based SMT system; Statistical Machine Translation) 에도 순방향 신경망 (feedfoward neural networks) 을 도입하려는 연구가 활발하다. 해당 논문에서는 전통적인 구 단위 SMT 시스템의 일부분으로 도입할 수 있는 신경망 구조를 제안하였다.\n","\n","해당 논문에서는 이 구조를 RNN 인코더-디코더 (RNN Encoder-Decoder) 라고 지칭하며, 각각 인코더와 디코더로 동작하는 두 개의 RNN으로 이루어져 있다고 설명한다. 인코더 역할의 RNN은 가변 길이의 입력 시퀀스를 고정 길이의 벡터로 매핑하는 연산을 수행하고, 디코더 역할의 RNN은 이 벡터를 다시 가변 길이의 출력 시퀀스로 매핑한다. 두 RNN은 입력 시퀀스가 주어졌을 때 출력 시퀀스의 조건부 확률을 최대화하도록 \"함께\" 훈련된다.\n","\n","또한 해당 논문에서는 추후에 GRU (Gated Recurrent Unit) 라고 불리게 되눈 새로운 RNN 구조를 제시한다. 이 구조는 LSTM에서 영감을 받아 간소화한 것으로, 총 두 개의 게이트로 이루어져 있다. 그 중 하나인 리셋 게이트 (reset gate) 는 현재 입력으로 새로운 은닉 스테이트 (hidden state) 를 계산할 때 이전 은닉 스테이트를 얼마나 반영할 것인지 조절한다. 다른 하나인 업데이트 게이트 (update gate) 는 다음 은닉 스테이트를 계산하는 데에 있어서 이전 은닉 스테이트와 새로운 은닉 스테이트 간의 비중을 조절한다.\n","\n","제안한 방법을 WMT'14 English to French Machine Translation 작업에 적용하였다. SMT 시스템으로 기본 세팅을 이용한 Moses를 사용하였으며, 이와 함께 논문에서 제안한 구조를 사용했을 때 테스트 세트에서 33.87에 달하는 점수를 얻었다. (기존 시스템만 사용했을 때보다 0.57만큼 높은 점수이다.)\n","\n","(단, 해당 논문에서는 영어 문장을 프랑스어 문장으로 번역하는 확률을 학습시킨 것이 아니라, 영어 구를 프랑스어 구로 번역하는 확률을 학습시켰다.)"],"metadata":{"id":"J1ujXsdk8R1N"}},{"cell_type":"markdown","source":["<figure align=\"center\">\n","  <img src=\"https://drive.google.com/uc?export=view&id=1lz1k42N9XQSfaAb449bhXpCfPfpZm7nv\" width=800 />\n","  <figcaption>Encoder and Decoder Architecture</figcaption>\n","</figure>"],"metadata":{"id":"E4CToKcqHc1C"}},{"cell_type":"markdown","source":["<figure align=\"center\">\n","  <img src=\"https://drive.google.com/uc?export=view&id=1fwFTSzQm0yqZD_rWsISIePdg4UDsUcyo\" width=900 />\n","  <figcaption>Example of Network Flow</figcaption>\n","</figure>"],"metadata":{"id":"gK46-DsRKCS4"}},{"cell_type":"markdown","source":["# Models"],"metadata":{"id":"PileDUTe7Cq4"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn"],"metadata":{"id":"RbciNImiOUUt","executionInfo":{"status":"ok","timestamp":1650983268195,"user_tz":-540,"elapsed":3856,"user":{"displayName":"김승현","userId":"01812332458933298199"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## GRU Module"],"metadata":{"id":"X8CHRNUh7DvB"}},{"cell_type":"markdown","source":["<figure align=\"center\">\n","  <img src=\"https://drive.google.com/uc?export=view&id=1PL067WUxNtDMZiCBa2cGLWWdFMX-iabM\" width=350 />\n","  <figcaption>GRU Architecture [1]</figcaption>\n","</figure>"],"metadata":{"id":"X69HT8aGKaM8"}},{"cell_type":"markdown","source":["$$\n","r = \\sigma\\big(W_r\\cdot[x, h_{\\langle t-1\\rangle}]+b_r\\big), \\\\\n","z = \\sigma\\big(W_z\\cdot[x, h_{\\langle t-1\\rangle}]+b_z\\big), \\\\\n","\\tilde{h}^{\\langle t\\rangle} = \\tanh\\big(W\\cdot[x, r\\odot h_{\\langle t-1\\rangle}]+b_{\\tilde{h}}\\big), \\\\\n","h^{\\langle t\\rangle} = z \\odot h^{\\langle t-1\\rangle} + (1-z) \\odot \\tilde{h}^{\\langle t\\rangle}.\n","$$\n","<div align=\"center\">Basic Equations of GRU [2]</div>"],"metadata":{"id":"N2ENc-HBkqcF"}},{"cell_type":"markdown","source":["그러나 디코더에 쓰이는 GRU는 하나의 인자 (요약 벡터 $c$)를 더 입력받는다.\n","\n","$$\n","r = \\sigma\\big(W_r\\cdot[x, h_{\\langle t-1\\rangle}, c]+b_r\\big), \\\\\n","z = \\sigma\\big(W_z\\cdot[x, h_{\\langle t-1\\rangle}, c]+b_z\\big), \\\\\n","\\tilde{h}^{\\langle t\\rangle} = \\tanh\\big(W\\cdot[x, r\\odot h_{\\langle t-1\\rangle}, r\\odot c]+b_{\\tilde{h}}\\big), \\\\\n","h^{\\langle t\\rangle} = z \\odot h^{\\langle t-1\\rangle} + (1-z) \\odot \\tilde{h}^{\\langle t\\rangle}.\n","$$\n","<div align=\"center\">Equations of GRU for Decoder [2]</div>"],"metadata":{"id":"cze1Mt9ck2dY"}},{"cell_type":"code","source":["class GRULayer(nn.Module):\n","\n","  def __init__(self, input_size, hidden_size, is_decoder, dtype=torch.float, device='cpu'):\n","    super(GRULayer, self).__init__()\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    self.is_decoder = is_decoder\n","    self.factory_kwargs = {'dtype': dtype, 'device': device}\n","\n","    # summary_size == hidden_size\n","    combined_size = input_size + 2 * hidden_size if is_decoder \\\n","      else input_size + hidden_size\n","    self.linear_reset = nn.Linear(combined_size, hidden_size,\n","                                  **self.factory_kwargs)\n","    self.linear_update = nn.Linear(combined_size, hidden_size,\n","                                   **self.factory_kwargs)\n","    self.linear_new = nn.Linear(combined_size, hidden_size,\n","                                **self.factory_kwargs)\n","\n","  def forward(self, input, hidden=None, summary=None):\n","    \"\"\"Args:\n","        input: torch.Tensor, [seq_len, input_size] or\n","          [seq_len, batch_size, input_size]\n","        hidden: torch.Tensor, [hidden_size] or [batch_size, hidden_size]\n","        summary: torch.Tensor, [hidden_size] or [batch_size, hidden_size]\n","\n","    Return:\n","        output: torch.Tensor, [seq_len, hidden_size] or\n","            [seq_len, batch_size, hidden_size]\n","        hidden: torch.Tensor, [hidden_size] or [batch_size, hidden_size]\n","    \"\"\"\n","    assert (2 <= len(input.shape) <= 3) and input.size(-1) == self.input_size, \\\n","      \"The shape of the `input` should be [seq_len, input_size] or \" \\\n","      \"[seq_len, batch_size, input_size]\"\n","    assert (not self.is_decoder and summary is None) or \\\n","      (self.is_decoder and hidden is not None and summary is not None), \\\n","      \"The GRU for an encoder should not receive a summary vector and for \" \\\n","      \"a decoder should receive a hidden state and a summary vector.\"\n","    assert (hidden is None) or \\\n","      (len(hidden.shape) == len(input.shape) - 1 and \\\n","       hidden.size(-1) == self.hidden_size), \\\n","      \"The shape of the `hidden` should be [hidden_size] or \" \\\n","      \"[batch_size, hidden_size]\"\n","    assert (summary is None) or \\\n","      (len(summary.shape) == len(input.shape) - 1 and \\\n","       summary.size(-1) == self.hidden_size), \\\n","      \"The shape of the `summary` should be [hidden_size] or \" \\\n","      \"[batch_size, hidden_size]\"\n","    \n","    is_batched = len(input.shape) == 3\n","    if is_batched:\n","      seq_len, batch_size, _ = input.shape\n","      outputs = torch.zeros(seq_len, batch_size, self.hidden_size,\n","                            **self.factory_kwargs)\n","      if hidden is None:\n","        hidden = torch.zeros(batch_size, self.hidden_size,\n","                             **self.factory_kwargs)\n","    else:\n","      seq_len, _ = input.shape\n","      outputs = torch.zeros(seq_len, self.hidden_size,\n","                            **self.factory_kwargs)\n","      if hidden is None:\n","        hidden = torch.zeros(self.hidden_size,\n","                             **self.factory_kwargs)\n","    \n","    for i in range(seq_len):\n","      if self.is_decoder:\n","        combined = torch.cat((input[i], hidden, summary),\n","                             dim=len(input[i].shape)-1)\n","      else:\n","        combined = torch.cat((input[i], hidden), dim=len(input[i].shape)-1)\n","      reset = torch.sigmoid(self.linear_reset(combined))\n","      update = torch.sigmoid(self.linear_update(combined))\n","\n","      if self.is_decoder:\n","        combined = torch.cat((input[i], reset * hidden, reset * summary),\n","                             dim=len(input[i].shape)-1)\n","      else:\n","        combined = torch.cat((input[i], reset * hidden),\n","                             dim=len(input[i].shape)-1)\n","      new = torch.tanh(self.linear_new(combined))\n","      hidden = update * hidden + (1 - update) * new\n","\n","      outputs[i] = hidden\n","    \n","    return outputs, hidden"],"metadata":{"id":"YxhjzNk8OSD6","executionInfo":{"status":"ok","timestamp":1650984630091,"user_tz":-540,"elapsed":332,"user":{"displayName":"김승현","userId":"01812332458933298199"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["class GRU(nn.Module):\n","\n","  def __init__(self, input_size, hidden_size, num_layers, is_decoder, dtype=torch.float, device='cpu'):\n","    super(GRU, self).__init__()\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    self.num_layers = num_layers\n","    self.is_decoder = is_decoder\n","    self.factory_kwargs = {'dtype': dtype, 'device': device}\n","\n","    layers = \\\n","      [GRULayer(input_size, hidden_size, is_decoder, **self.factory_kwargs)] + \\\n","      [GRULayer(hidden_size, hidden_size, is_decoder, **self.factory_kwargs)\n","      for _ in range(num_layers - 1)]\n","    self.layers = nn.ModuleList(layers)\n","\n","  def forward(self, input, hiddens=None, summarys=None):\n","    \"\"\"Args:\n","        input: torch.Tensor, [seq_len, input_size] or\n","          [seq_len, batch_size, input_size]\n","        hiddens: torch.Tensor, [num_layers, hidden_size] or\n","          [num_layers, batch_size, hidden_size]\n","        summarys: torch.Tensor, [num_layers, hidden_size] or\n","          [num_layers, batch_size, hidden_size]\n","\n","    Return:\n","        output: torch.Tensor, [seq_len, hidden_size] or\n","            [seq_len, batch_size, hidden_size]\n","        hidden: torch.Tensor, [num_layers, hidden_size] or\n","          [num_layers, batch_size, hidden_size]\n","    \"\"\"\n","    assert (2 <= len(input.shape) <= 3) and input.size(-1) == self.input_size, \\\n","      \"The shape of the `input` should be [seq_len, input_size] or \" \\\n","      \"[seq_len, batch_size, input_size]\"\n","    assert (not self.is_decoder and summarys is None) or \\\n","      (self.is_decoder and hiddens is not None and summarys is not None), \\\n","      \"The GRU for an encoder should not receive a summary vector and for \" \\\n","      \"a decoder should receive a hidden state and a summary vector.\"\n","    assert (hiddens is None) or \\\n","      (len(hiddens.shape) == len(input.shape) and \\\n","       hiddens.size(0) == self.num_layers and \\\n","       hiddens.size(-1) == self.hidden_size), \\\n","      \"The shape of the `hidden` should be [num_layers, hidden_size] or \" \\\n","      \"[num_layers, batch_size, hidden_size]\"\n","    assert (summarys is None) or \\\n","      (len(summarys.shape) == len(input.shape) and \\\n","       summarys.size(0) == self.num_layers and \\\n","       summarys.size(-1) == self.hidden_size), \\\n","      \"The shape of the `summary` should be [num_layers, hidden_size] or \" \\\n","      \"[num_layers, batch_size, hidden_size]\"\n","\n","    is_batched = len(input.shape) == 3\n","    if is_batched:\n","      seq_len, batch_size, _ = input.shape\n","      if hiddens is None:\n","        hiddens = torch.zeros(self.num_layers, batch_size, self.hidden_size,\n","                              **self.factory_kwargs)\n","    else:\n","      seq_len, _ = input.shape\n","      if hiddens is None:\n","        hiddens = torch.zeros(self.num_layers, self.hidden_size,\n","                              **self.factory_kwargs)\n","\n","    output = input\n","    next_hiddens = torch.zeros_like(hiddens)\n","    for i in range(self.num_layers):\n","      if self.is_decoder:\n","        output, hidden = self.layers[i](output, hiddens[i], summarys[i])\n","      else:\n","        output, hidden = self.layers[i](output, hiddens[i])\n","      next_hiddens[i] = hidden\n","\n","    return output, next_hiddens"],"metadata":{"id":"7_vWvPsQOqj5","executionInfo":{"status":"ok","timestamp":1650983814310,"user_tz":-540,"elapsed":10,"user":{"displayName":"김승현","userId":"01812332458933298199"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["## Encoder Module"],"metadata":{"id":"l64McN027GDd"}},{"cell_type":"code","source":["class Encoder(nn.Module):\n","\n","  def __init__(self, input_size, embed_size, hidden_size, num_rnn_layers,\n","               padding_index, dtype=torch.float, device='cpu'):\n","    super(Encoder, self).__init__()\n","    self.input_size = input_size\n","    self.embed_size = embed_size\n","    self.hidden_size = hidden_size\n","    self.num_rnn_layers = num_rnn_layers\n","    self.factory_kwargs = {'dtype': dtype, 'device': device}\n","\n","    self.embedding = nn.Embedding(input_size, embed_size, padding_index,\n","                                  **self.factory_kwargs)\n","    self.rnn = GRU(embed_size, hidden_size, num_rnn_layers, is_decoder=False,\n","                    **self.factory_kwargs)\n","    self.linear_summary = nn.Linear(hidden_size, hidden_size,\n","                                    **self.factory_kwargs)\n","\n","  def forward(self, input, hidden=None):\n","    \"\"\"Args:\n","        input: torch.Tensor, [seq_len] or [seq_len, batch_size]\n","        hidden (optional): torch.Tensor, [num_rnn_layers, hidden_size] or\n","          [num_rnn_layers, batch_size, hidden_size]\n","\n","    Return:\n","        output: torch.Tensor, [seq_len, hidden_size] or\n","            [seq_len, batch_size, hidden_size]\n","        hidden: torch.Tensor, [num_rnn_layers, hidden_size] or\n","          [num_rnn_layers, batch_size, hidden_size]\n","        summary: torch.Tensor, [num_rnn_layers, hidden_size] or\n","          [num_rnn_layers, batch_size, hidden_size]\n","    \"\"\"\n","    embedded = self.embedding(input)\n","    output, hidden = self.rnn(embedded, hidden)\n","    summary = torch.tanh(self.linear_summary(hidden))\n","    return output, hidden, summary"],"metadata":{"id":"T_4fZTz6mP75","executionInfo":{"status":"ok","timestamp":1650983268654,"user_tz":-540,"elapsed":11,"user":{"displayName":"김승현","userId":"01812332458933298199"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Decoder Module"],"metadata":{"id":"OjrqnGAU7HgK"}},{"cell_type":"code","source":["class Decoder(nn.Module):\n","\n","  def __init__(self, embed_size, hidden_size, output_size, num_rnn_layers,\n","               padding_index, dtype=torch.float, device='cpu'):\n","    super(Decoder, self).__init__()\n","    self.embed_size = embed_size\n","    self.hidden_size = hidden_size\n","    self.output_size = output_size\n","    self.num_rnn_layers = num_rnn_layers\n","    self.num_maxouts = 500\n","    self.pool_size = 2\n","    self.stride = 2\n","    self.factory_kwargs = {'dtype': dtype, 'device': device}\n","\n","    input_size = output_size\n","    self.embedding = nn.Embedding(input_size, embed_size, padding_index,\n","                                  **self.factory_kwargs)\n","    self.linear_hidden = nn.Linear(hidden_size, hidden_size,\n","                                   **self.factory_kwargs)\n","    self.rnn = GRU(embed_size, hidden_size, num_rnn_layers, is_decoder=True,\n","                    **self.factory_kwargs)\n","    # 아래에서 input_size 대신 embed_size를 했는데 괜찮을까?\n","    self.linear_maxout = nn.Linear(embed_size + 2 * hidden_size,\n","                                    self.num_maxouts * self.pool_size,\n","                                    **self.factory_kwargs)\n","    self.linear_output = nn.Linear(self.num_maxouts, output_size,\n","                                   **self.factory_kwargs)\n","\n","  def forward(self, input, hidden=None, summary=None, max_len=50,\n","              teacher_forcing_ratio=0.):\n","    \"\"\"Args:\n","        input: torch.Tensor, [seq_len] or [seq_len, batch_size]\n","        hidden: torch.Tensor, [num_layers, hidden_size] or\n","          [num_layers, batch_size, hidden_size]\n","        summary: torch.Tensor, [num_layers, hidden_size] or\n","          [num_layers, batch_size, hidden_size]\n","        max_len (optional): a non-negative integer\n","        teacher_forcing_ratio (optional): a float number between 0 and 1\n","\n","    Return:\n","        output: torch.Tensor, [max_len, output_size] or\n","            [max_len, batch_size, output_size]\n","        hidden: torch.Tensor, [num_layers, hidden_size] or\n","          [num_layers, batch_size, hidden_size]\n","        summary: torch.Tensor, [num_layers, hidden_size] or\n","          [num_layers, batch_size, hidden_size]\n","    \"\"\"\n","    #TODO: sample until all rows have more than one EOS\n","    # input.size(0) == target length\n","    if self.training: max_len = input.size(0)\n","      \n","    is_batched = len(input.shape) == 2\n","    if is_batched:\n","      _, batch_size = input.shape\n","      outputs = torch.zeros(max_len, batch_size, self.output_size,\n","                            **self.factory_kwargs)\n","    else:\n","      outputs = torch.zeros(max_len, self.output_size, **self.factory_kwargs)\n","\n","    assert summary is not None, \"You should give summary vector into the \" \\\n","      \"decoder\"\n","    if hidden is None:\n","      hidden = torch.tanh(self.linear_hidden(summary))\n","\n","    inputs = input\n","    input_shape = (1, batch_size) if is_batched else (1,)\n","    input = inputs[0].view(input_shape) # [1] or [1, batch_size]\n","    for i in range(1, max_len):\n","      embedded = self.embedding(input)\n","      output, hidden = self.rnn(embedded, hidden, summary)\n","      combined = torch.cat((hidden[-1], embedded[0], summary[-1]),\n","                            dim=len(hidden.shape)-2)\n","      # [batch_size, embed_size + 2 * hidden_size]\n","      # -> [batch_size, self.num_maxouts]\n","      maxout = nn.functional.max_pool1d(self.linear_maxout(combined),\n","                                        kernel_size=self.pool_size,\n","                                        stride=self.stride)\n","      output = self.linear_output(maxout) # [batch_size, output_size]\n","      outputs[i] = output.view(outputs.shape[1:])\n","      if self.training and torch.randn(1) < teacher_forcing_ratio:\n","        # use teacher forcing\n","        input = inputs[i].view(input_shape)\n","      else:\n","        # do not use teacher forcing\n","        input = output.argmax(len(input.shape)-1).view(input_shape)\n","          \n","    return outputs, hidden, summary"],"metadata":{"id":"cCWyHQmgmv1g","executionInfo":{"status":"ok","timestamp":1650984400959,"user_tz":-540,"elapsed":349,"user":{"displayName":"김승현","userId":"01812332458933298199"}}},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":["## A Whole Seq2Seq Module"],"metadata":{"id":"GfuseFTT7Ig3"}},{"cell_type":"code","source":["class Seq2SeqNetwork(nn.Module):\n","\n","  def __init__(self, input_size, embed_size, hidden_size, output_size,\n","               num_rnn_layers, padding_index, dtype=torch.float, device='cpu'):\n","    super(Seq2SeqNetwork, self).__init__()\n","    self.input_size = input_size\n","    self.embed_size = embed_size\n","    self.hidden_size = hidden_size\n","    self.output_size = output_size\n","    self.num_rnn_layers = num_rnn_layers\n","    self.factory_kwargs = {'dtype': dtype, 'device': device}\n","\n","    self.encoder = Encoder(input_size, embed_size, hidden_size, num_rnn_layers,\n","                           padding_index, **self.factory_kwargs)\n","    self.decoder = Decoder(embed_size, hidden_size, output_size, num_rnn_layers,\n","                           padding_index, **self.factory_kwargs)\n","\n","  def forward(self, src, trg, max_len=50, teacher_forcing_ratio=0.):\n","    \"\"\"Args:\n","        src: torch.Tensor, [src_len] or [src_len, batch_size]\n","        trg: torch.Tensor, [trg_len] or [trg_len, batch_size]\n","        max_len (optional): a non-negative integer\n","        teacher_forcing_ratio (optional): a float number between 0 and 1\n","\n","    Return:\n","        output: torch.Tensor, [trg_len, output_size] or\n","            [trg_len, batch_size, output_size]\n","    \"\"\"\n","    _, _, summary = self.encoder(src)\n","    output, _, _ = self.decoder(trg, summary=summary, max_len=max_len,\n","                             teacher_forcing_ratio=teacher_forcing_ratio)\n","    return output\n","\n","  def encode(self, input, hidden=None):\n","    \"\"\"Args:\n","        input: torch.Tensor, [seq_len] or [seq_len, batch_size]\n","        hidden: torch.Tensor, [num_layers, hidden_size] or\n","          [num_layers, batch_size, hidden_size]\n","\n","    Return:\n","        output: torch.Tensor, [seq_len, hidden_size] or\n","            [trg_len, batch_size, hidden_size]\n","        hidden: torch.Tensor, [num_layers, hidden_size] or\n","          [num_layers, batch_size, hidden_size]\n","        summary: torch.Tensor, [num_layers, hidden_size] or\n","          [num_layers, batch_size, hidden_size]\n","    \"\"\"\n","    return self.encoder(input, hidden)\n","\n","  def decode(self, input, hidden=None, summary=None, beam_size=1, max_len=50,\n","            teacher_forcing_ratio=0.):\n","    \"\"\"Args:\n","        input: torch.Tensor, [seq_len] or [seq_len, batch_size]\n","        beam_size (optional): a non-negative integer\n","        max_len (optional): a non-negative integer\n","        teacher_forcing_ratio (optional): a float number between 0 and 1\n","\n","    Return:\n","        output: torch.Tensor, [max_len, output_size] or\n","            [max_len, batch_size, output_size]\n","    \"\"\"\n","    output, _ = self.decoder(input, hidden, summary, max_len,\n","                             teacher_forcing_ratio)\n","    return output"],"metadata":{"id":"R2s6dKYxmxn1","executionInfo":{"status":"ok","timestamp":1650983268655,"user_tz":-540,"elapsed":9,"user":{"displayName":"김승현","userId":"01812332458933298199"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# Temp"],"metadata":{"id":"f7xpKkouQMRj"}},{"cell_type":"code","source":["import torch.optim as optim\n","import math, time"],"metadata":{"id":"_dhC8LVIQTfn","executionInfo":{"status":"ok","timestamp":1650983268655,"user_tz":-540,"elapsed":8,"user":{"displayName":"김승현","userId":"01812332458933298199"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def format_time(start_time, current_time, progress):\n","  elapsed = int(current_time - start_time)\n","  elapsed_time = f'{elapsed // 60:2d}m {elapsed % 60:2d}s'\n","  total = int(elapsed / progress)\n","  total_time = f'{total // 60:2d}m {total % 60:2d}s'\n","  return elapsed_time, total_time\n","\n","def train(dataloader, model, optimizer, loss_fn, verbose=True, print_every=50):\n","  model.train()\n","  avg_loss = 0.\n","  loss_history = []\n","  model.train()\n","  for batch, (src, trg) in enumerate(dataloader):\n","    src, trg = src.to(device), trg.to(device)\n","    pred = model(src, trg)\n","    pred = pred[1:].view(-1, pred.size(2))\n","    trg = trg[1:].view(-1)\n","    # pred: [trg_len * batch_size, output_size], trg: [trg_len * batch_size]\n","    loss = loss_fn(pred, trg)\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n","    optimizer.step()\n","\n","    avg_loss += loss.item()\n","    loss_history.append(loss.item())\n","    if verbose and batch % print_every == 0 and batch > 0:\n","      avg_loss /= print_every\n","      print(f'> [{(batch + 1) * src.size(1):5d}/{len(dataloader.dataset):5d}]',\n","            f'loss={avg_loss:1.4f}, ppl={math.exp(avg_loss):7.3f}')\n","      avg_loss = 0.\n","    \n","  return loss_history\n","\n","def evaluate(dataloader, model, loss_fn, verbose=True):\n","  avg_loss = 0.\n","  loss_history = []\n","  model.eval()\n","  with torch.no_grad():\n","    for batch, (src, trg) in enumerate(dataloader):\n","      src, trg = src.to(device), trg.to(device)\n","      pred = model(src, trg)\n","      pred = pred[1:trg.size(0)].view(-1, pred.size(2))\n","      trg = trg[1:].view(-1)\n","      # pred: [trg_len * batch_size, output_size], trg: [trg_len * batch_size]\n","      loss = loss_fn(pred, trg)\n","\n","      avg_loss += loss.item()\n","      loss_history.append(loss.item())\n","  if verbose:\n","    avg_loss /= len(dataloader)\n","    print(f'> [evaluation]  loss={avg_loss:1.4f}, ',\n","          f'ppl={math.exp(avg_loss):7.3f}')\n","  return avg_loss, loss_history"],"metadata":{"id":"efRRRG5DQVd4","executionInfo":{"status":"ok","timestamp":1650983268966,"user_tz":-540,"elapsed":318,"user":{"displayName":"김승현","userId":"01812332458933298199"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["input_size = 8000\n","output_size = 6000\n","padding_index = 1\n","device = 'cuda' if torch.cuda.is_available else 'cpu'\n","\n","BATCH_SIZE = 64\n","\n","EMBED_SIZE = 512\n","HIDDEN_SIZE = 512\n","NUM_LAYERS = 4\n","\n","NUM_EPOCHS = 1\n","# LEARNING_RATE = 0.001\n","MAX_GRAD_NORM = 5"],"metadata":{"id":"WagE5euZQeyJ","executionInfo":{"status":"ok","timestamp":1650983268967,"user_tz":-540,"elapsed":5,"user":{"displayName":"김승현","userId":"01812332458933298199"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["model = Seq2SeqNetwork(input_size, EMBED_SIZE, HIDDEN_SIZE, output_size,\n","                       NUM_LAYERS, padding_index, device=device)\n","for param in model.parameters():\n","  param.data.normal_(std=0.01)\n","print(f'The number of model parameter: {sum([param.numel() for param in model.parameters()]):,}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NVh9p-smQkM8","executionInfo":{"status":"ok","timestamp":1650984634968,"user_tz":-540,"elapsed":329,"user":{"displayName":"김승현","userId":"01812332458933298199"}},"outputId":"8c974cb7-6c12-41da-cac5-eb7e5f57cc49"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of model parameter: 27,977,240\n"]}]},{"cell_type":"code","source":["import torch.optim as optim\n","# Prepare an optimizer and a loss function\n","optimizer = optim.Adam(model.parameters())\n","loss_fn = nn.CrossEntropyLoss(ignore_index=padding_index)\n","# reduction='mean' -> the gradients are summed and divided by batch_size"],"metadata":{"id":"4KEvcdKMQoLX","executionInfo":{"status":"ok","timestamp":1650984635806,"user_tz":-540,"elapsed":3,"user":{"displayName":"김승현","userId":"01812332458933298199"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["import time\n","\n","src_train = torch.randint(0, input_size, (30, BATCH_SIZE))\n","trg_train = torch.randint(0, output_size, (35, BATCH_SIZE))\n","src_val = torch.randint(0, input_size, (30, BATCH_SIZE))\n","trg_val = torch.randint(0, output_size, (35, BATCH_SIZE))\n","\n","best_val_loss = float('inf')\n","train_loss_history = []\n","val_loss_history = []\n","start_time = time.time()\n","for epoch in range(1, NUM_EPOCHS + 1):\n","  print('-' * 41)\n","  print(f'Epoch {epoch}')\n","  print('-' * 41)\n","\n","  with torch.autograd.set_detect_anomaly(True):\n","    train_loss_history += train([(src_train, trg_train)], model, optimizer, loss_fn)\n","  val_loss, loss_history = evaluate([(src_val, trg_val)], model, loss_fn)\n","  val_loss_history += loss_history\n","  \n","  if epoch > NUM_EPOCHS // 2 and val_loss < best_val_loss:\n","    best_val_loss = val_loss\n","    torch.save(model.state_dict(), f'seq2seq-{val_loss * pow(10, 5):07.0f}.pth')\n","  \n","  elapsed_time, total_time = format_time(start_time, time.time(), epoch / NUM_EPOCHS)\n","  print('-' * 41)\n","  print(f'End of epoch {epoch} ({elapsed_time}/{total_time})')\n","  print('-' * 41)\n","  print()\n","print('Done!')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jMnAe-T0QwGd","executionInfo":{"status":"ok","timestamp":1650984687964,"user_tz":-540,"elapsed":6182,"user":{"displayName":"김승현","userId":"01812332458933298199"}},"outputId":"217e880e-2022-40f9-cbf1-7c64e0f33599"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["-----------------------------------------\n","Epoch 1\n","-----------------------------------------\n","> [evaluation]  loss=8.7000,  ppl=6003.026\n","-----------------------------------------\n","End of epoch 1 ( 0m  5s/ 0m  5s)\n","-----------------------------------------\n","\n","Done!\n"]}]},{"cell_type":"markdown","source":["# References"],"metadata":{"id":"t-0uHJW46MLB"}},{"cell_type":"markdown","source":["[1] Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation [[link]](https://doi.org/10.48550/arXiv.1406.1078)"],"metadata":{"id":"qZJ81DNG6TZi"}}]}