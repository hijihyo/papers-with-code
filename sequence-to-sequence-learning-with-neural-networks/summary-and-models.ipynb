{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"summary-and-models.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPlLWpR1u0oST6Rtc8I8JO8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Sequence to Sequence Learning with Neural Networks [1]\n","\n","## Part 1. 논문 정리와 모델 구현"],"metadata":{"id":"Co0iMgnI58Do"}},{"cell_type":"markdown","source":["# Summary"],"metadata":{"id":"NBZyW0Hq7Bgd"}},{"cell_type":"markdown","source":["Deep Neural Networks (DNNs) 는 입력과 출력 데이터를 고정 길이의 벡터로 인코딩하는 방식으로 음성 인식과 객체 탐지 등의 분야에서 우수한 성과를 내고 있었다. 그러나 이러한 방법론은 길이가 제각각인 시퀀스 처리에는 도입하기 어렵다는 한계가 있었다. 해당 논문에서는 일반적인 시퀀스 처리에 도입할 수 있도록 두 개의 Recurrent Neural Networks (RNNs) 을 사용한 네트워크 구조를 제안\b하였다.\n","\n","이 네트워크는 두 개의 RNNs로 이루어져 있다. 하나는 한 번에 한 단위씩 (논문에서는 단어 기준) 읽는 방식으로 여러 번에 걸쳐 하나의 시퀀스를 입력받아 고정 길이의 벡터로 인코딩한다. 다른 하나는 인코딩된 벡터를 읽고 한 번에 한 단위씩 출력하는 방식으로 하나의 시퀀스를 만들어낸다.\n","\n","해당 논문에서는 Long Short-Term Memory (LSTM) 이 긴 시간 동안의 의존성이 필요한 문제에서 유용하다는 점에 착안하여 네트워크의 RNN 구조로 LSTM을 선택하였다. (그리고 [2]에서 제안한 LSTM을 사용하였다.)\n","\n","그리고 제안한 방법을 WMT'14 English to French Machine Translation 작업에 적용하였다. 그 결과 전체 테스트 세트에서 34.8에 달하는 BLEU 점수를 얻었다."],"metadata":{"id":"J1ujXsdk8R1N"}},{"cell_type":"markdown","source":["<figure align=\"center\">\n","  <img src=\"https://drive.google.com/uc?export=view&id=1dDZOhoDTm_m8kgyXr2qDZ67UQ2wSmU3X\" width=300 />\n","  <figcaption>Encoder and Decoder Architecture</figcaption>\n","</figure>"],"metadata":{"id":"E4CToKcqHc1C"}},{"cell_type":"markdown","source":["<figure align=\"center\">\n","  <img src=\"https://drive.google.com/uc?export=view&id=1fwFTSzQm0yqZD_rWsISIePdg4UDsUcyo\" width=900 />\n","  <figcaption>Example of Sequence to Sequence Network Flow</figcaption>\n","</figure>"],"metadata":{"id":"gK46-DsRKCS4"}},{"cell_type":"markdown","source":["# Models"],"metadata":{"id":"PileDUTe7Cq4"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn"],"metadata":{"id":"RbciNImiOUUt","executionInfo":{"status":"ok","timestamp":1650883560821,"user_tz":-540,"elapsed":456,"user":{"displayName":"김승현","userId":"01812332458933298199"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["## LSTM Module"],"metadata":{"id":"X8CHRNUh7DvB"}},{"cell_type":"markdown","source":["<figure align=\"center\">\n","  <img src=\"https://drive.google.com/uc?export=view&id=19Au229q45Z6hCb6qMFepzRKsl_4RrRyw\" width=400 />\n","  <figcaption>LSTM Architecture [2]</figcaption>\n","</figure>"],"metadata":{"id":"X69HT8aGKaM8"}},{"cell_type":"markdown","source":["$$\n","i_t = \\sigma(W_{xi}x_t + W_{hi}h_{t-1} + W_{ci}c_{t-1} + b_i) \\\\\n","f_t = \\sigma(W_{xf}x_t + W_{hf}h_{t-1} + W_{cf}c_{t-1} + b_f) \\\\\n","c_t = f_t c_{t-1} + i_t \\tanh(W_{xc}x_t + W_{hc}h_{t-1} + b_c) \\\\\n","o_t = \\sigma(W_{xo}x_t + W_{ho}h_{t-1} + W_{co}c_t + b_o) \\\\\n","h_t = o_t \\tanh(c_t) \\\\\n","\\quad\\\\\n","\\text{The above equations are equivalent to:} \\\\\n","\\quad\\\\\n","i_t = \\sigma(W_i \\cdot [x_t, h_{t-1}, c_{t-1}] + b_i) \\\\\n","f_t = \\sigma(W_f \\cdot [x_t, h_{t-1}, c_{t-1}] + b_f) \\\\\n","c_t = f_t c_{t-1} + i_t \\tanh(W_c \\cdot [x_t, h_{t-1}] + b_c) \\\\\n","o_t = \\sigma(W_o \\cdot [x_t, h_{t-1}, c_t] + b_o) \\\\\n","h_t = o_t \\tanh(c_t)\n","$$\n","<div align=\"center\">Equations of LSTM [2]</div>"],"metadata":{"id":"Lqebo-NoLJrr"}},{"cell_type":"code","source":["class LSTMLayer(nn.Module):\n","\n","  def __init__(self, input_size, hidden_size, dtype=torch.float, device='cpu'):\n","    super(LSTMLayer, self).__init__()\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    self.factory_kwargs = {'dtype': dtype, 'device': device}\n","\n","    # cell_size == hidden_size\n","    self.linear_write = nn.Linear(input_size + 2 * hidden_size, hidden_size,\n","                                  **self.factory_kwargs)\n","    self.linear_forget = nn.Linear(input_size + 2 * hidden_size, hidden_size,\n","                                  **self.factory_kwargs)\n","    self.linear_cell = nn.Linear(input_size + hidden_size, hidden_size,\n","                                  **self.factory_kwargs)\n","    self.linear_output = nn.Linear(input_size + 2 * hidden_size, hidden_size,\n","                                  **self.factory_kwargs)\n","\n","  def forward(self, input, states=None):\n","    \"\"\"Args:\n","        input: torch.Tensor, [seq_len, input_size] or\n","          [seq_len, batch_size, input_size]\n","        states (optional): a tuple of two torch.Tensor\n","            hidden: torch.Tensor, [hidden_size]  or [batch_size, hidden_size]\n","            cell: torch.Tensor, [hidden_size] or [batch_size, hidden_size]\n","\n","    Return:\n","        output: torch.Tensor, [seq_len, hidden_size] or\n","            [seq_len, batch_size, hidden_size]\n","        states: a tuple of two torch.Tensor\n","            hidden: torch.Tensor, [hidden_size] or [batch_size, hidden_size]\n","            cell: torch.Tensor, [hidden_size] or [batch_size, hidden_size]\n","    \"\"\"\n","    assert (2 <= len(input.shape) <= 3) and input.size(-1) == self.input_size, \\\n","      \"The shape of the `input` should be [seq_len, input_size] or \" \\\n","      \"[seq_len, batch_size, input_size]\"\n","\n","    is_batched = len(input.shape) == 3\n","    if is_batched:\n","      seq_len, batch_size, _ = input.shape\n","      outputs = torch.zeros(seq_len, batch_size, self.hidden_size,\n","                            **self.factory_kwargs)\n","      if states is None:\n","        hidden = torch.zeros(batch_size, self.hidden_size,\n","                             **self.factory_kwargs)\n","        cell = torch.zeros(batch_size, self.hidden_size, **self.factory_kwargs)\n","      else:\n","        hidden, cell = states\n","    else:\n","      seq_len, _ = input.shape\n","      outputs = torch.zeros(seq_len, self.hidden_size, **self.factory_kwargs)\n","      if states is None:\n","        hidden = torch.zeros(self.hidden_size, **self.factory_kwargs)\n","        cell = torch.zeros(self.hidden_size, **self.factory_kwargs)\n","      else:\n","        hidden, cell = states\n","\n","    assert (1 <= len(hidden.shape) <= 2) and \\\n","      hidden.size(-1) == self.hidden_size, \\\n","      \"The shape of the `hidden` should be [hidden_size] or \" \\\n","      \"[batch_size, hidden_size]\"\n","    assert (1 <= len(cell.shape) <= 2) and \\\n","      cell.size(-1) == self.hidden_size, \\\n","      \"The shape of the `cell` should be [hidden_size] or \" \\\n","      \"[batch_size, hidden_size]\"\n","    \n","    seq_len = input.size(0)\n","    for i in range(seq_len):\n","      # input becomes [input_size] or [batch_size, input_size]\n","      combined = torch.cat((input[i], hidden, cell), dim=len(input[i].shape)-1)\n","      write = torch.sigmoid(self.linear_write(combined))\n","      forget = torch.sigmoid(self.linear_forget(combined))\n","\n","      combined = torch.cat((input[i], hidden), dim=len(input[i].shape)-1)\n","      cell = forget * cell + write * torch.tanh(self.linear_cell(combined))\n","      \n","      combined = torch.cat((input[i], hidden, cell),\n","                           dim=len(input[i].shape)-1)\n","      output = torch.sigmoid(self.linear_output(combined))\n","      hidden = output * torch.tanh(cell)\n","      outputs[i] = hidden\n","\n","    return outputs, (hidden, cell)"],"metadata":{"id":"YxhjzNk8OSD6","executionInfo":{"status":"ok","timestamp":1650883561398,"user_tz":-540,"elapsed":580,"user":{"displayName":"김승현","userId":"01812332458933298199"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["class LSTM(nn.Module):\n","\n","  def __init__(self, input_size, hidden_size, num_layers, dtype=torch.float, device='cpu'):\n","    super(LSTM, self).__init__()\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    self.num_layers = num_layers\n","    self.factory_kwargs = {'dtype': dtype, 'device': device}\n","\n","    layers = [LSTMLayer(input_size, hidden_size, **self.factory_kwargs)] + \\\n","      [LSTMLayer(hidden_size, hidden_size, **self.factory_kwargs)\n","      for _ in range(num_layers - 1)]\n","    self.layers = nn.ModuleList(layers)\n","\n","  def forward(self, input, states=None):\n","    \"\"\"Args:\n","        input: torch.Tensor, [seq_len, input_size] or\n","          [seq_len, batch_size, input_size]\n","        states (optional): a tuple of two torch.Tensor\n","            hidden: torch.Tensor, [num_layers, hidden_size] or\n","                [num_layers, batch_size, hidden_size]\n","            cell: torch.Tensor, [num_layers, hidden_size] or\n","                [num_layers, batch_size, hidden_size]\n","\n","    Return:\n","        output: torch.Tensor, [seq_len, hidden_size] or\n","            [seq_len, batch_size, hidden_size]\n","        states: a tuple of two torch.Tensor\n","            hidden: torch.Tensor, [num_layers, hidden_size] or\n","                [num_layers, batch_size, hidden_size]\n","            cell: torch.Tensor, [num_layers, hidden_size] or\n","                [num_layers, batch_size, hidden_size]\n","    \"\"\"\n","    assert (2 <= len(input.shape) <= 3) and input.size(-1) == self.input_size, \\\n","      \"The shape of the `input` should be [seq_len, input_size] or \" \\\n","      \"[seq_len, batch_size, input_size]\"\n","\n","    is_batched = len(input.shape) == 3\n","    if is_batched:\n","      seq_len, batch_size, _ = input.shape\n","      if states is None:\n","        hiddens = torch.zeros(self.num_layers, batch_size, self.hidden_size,\n","                             **self.factory_kwargs)\n","        cells = torch.zeros(self.num_layers, batch_size, self.hidden_size,\n","                           **self.factory_kwargs)\n","      else:\n","        hiddens, cells = states\n","    else:\n","      seq_len, _ = input.shape\n","      if states is None:\n","        hiddens = torch.zeros(self.num_layers, self.hidden_size,\n","                             **self.factory_kwargs)\n","        cells = torch.zeros(self.num_layers, self.hidden_size,\n","                           **self.factory_kwargs)\n","      else:\n","        hiddens, cells = states\n","\n","    assert (2 <= len(hiddens.shape) <= 3) and \\\n","      hiddens.size(0) == self.num_layers and \\\n","      hiddens.size(-1) == self.hidden_size, \\\n","      \"The shape of the `hidden` should be [num_layers, hidden_size] or \" \\\n","      \"[num_layers, batch_size, hidden_size]\"\n","    assert (2 <= len(cells.shape) <= 3) and \\\n","      cells.size(0) == self.num_layers and \\\n","      cells.size(-1) == self.hidden_size, \\\n","      \"The shape of the `cell` should be [num_layers, hidden_size] or \" \\\n","      \"[num_layers, batch_size, hidden_size]\"\n","    \n","    next_hiddens = torch.zeros_like(hiddens)\n","    next_cells = torch.zeros_like(cells)\n","    \n","    output = input\n","    for i in range(self.num_layers):\n","      # hidden and cell are [hidden_size] or [batch_size, hidden_size]\n","      output, (hidden, cell) = self.layers[i](output, (hiddens[i], cells[i]))\n","      next_hiddens[i], next_cells[i] = hidden, cell\n","\n","    return output, (next_hiddens, next_cells)"],"metadata":{"id":"7_vWvPsQOqj5","executionInfo":{"status":"ok","timestamp":1650883561398,"user_tz":-540,"elapsed":10,"user":{"displayName":"김승현","userId":"01812332458933298199"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["## Encoder Module"],"metadata":{"id":"l64McN027GDd"}},{"cell_type":"code","source":["class Encoder(nn.Module):\n","\n","  def __init__(self, input_size, embed_size, hidden_size, num_rnn_layers,\n","               padding_index, dtype=torch.float, device='cpu'):\n","    super(Encoder, self).__init__()\n","    self.input_size = input_size\n","    self.embed_size = embed_size\n","    self.hidden_size = hidden_size\n","    self.num_rnn_layers = num_rnn_layers\n","    self.factory_kwargs = {'dtype': dtype, 'device': device}\n","\n","    self.embedding = nn.Embedding(input_size, embed_size, padding_index,\n","                                  **self.factory_kwargs)\n","    self.rnn = LSTM(embed_size, hidden_size, num_rnn_layers,\n","                    **self.factory_kwargs)\n","\n","  def forward(self, input, states=None):\n","    \"\"\"Args:\n","        input: torch.Tensor, [seq_len] or [seq_len, batch_size]\n","        states (optional): a tuple of two torch.Tensor\n","            hidden: torch.Tensor, [num_rnn_layers, hidden_size] or\n","                [num_rnn_layers, batch_size, hidden_size]\n","            cell: torch.Tensor, [num_rnn_layers, hidden_size] or\n","                [num_rnn_layers, batch_size, hidden_size]\n","\n","    Return:\n","        output: torch.Tensor, [seq_len, hidden_size] or\n","            [seq_len, batch_size, hidden_size]\n","        states: a tuple of two torch.Tensor\n","            hidden: torch.Tensor, [num_rnn_layers, hidden_size] or\n","                [num_rnn_layers, batch_size, hidden_size]\n","            cell: torch.Tensor, [num_rnn_layers, hidden_size] or\n","                [num_rnn_layers, batch_size, hidden_size]\n","    \"\"\"\n","    embedded = self.embedding(input)\n","    output, (hidden, cell) = \\\n","      self.rnn(embedded) if states is None else self.rnn(embedded, states)\n","    return output, (hidden, cell)"],"metadata":{"id":"T_4fZTz6mP75","executionInfo":{"status":"ok","timestamp":1650883561399,"user_tz":-540,"elapsed":10,"user":{"displayName":"김승현","userId":"01812332458933298199"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["## Decoder Module"],"metadata":{"id":"OjrqnGAU7HgK"}},{"cell_type":"code","source":["class Decoder(nn.Module):\n","\n","  def __init__(self, embed_size, hidden_size, output_size, num_rnn_layers,\n","               padding_index, dtype=torch.float, device='cpu'):\n","    super(Decoder, self).__init__()\n","    self.embed_size = embed_size\n","    self.hidden_size = hidden_size\n","    self.output_size = output_size\n","    self.num_rnn_layers = num_rnn_layers\n","    self.factory_kwargs = {'dtype': dtype, 'device': device}\n","\n","    input_size = output_size\n","    self.embedding = nn.Embedding(input_size, embed_size, padding_index,\n","                                  **self.factory_kwargs)\n","    self.rnn = LSTM(embed_size, hidden_size, num_rnn_layers,\n","                    **self.factory_kwargs)\n","    self.linear = nn.Linear(hidden_size, output_size, **self.factory_kwargs)\n","    # linear layer가 들어가는게 맞나?\n","\n","  def forward(self, input, states=None, beam_size=1, max_len=50,\n","              teacher_forcing_ratio=0.):\n","    \"\"\"Args:\n","        input: torch.Tensor, [seq_len] or [seq_len, batch_size]\n","        states (optional): a tuple of two torch.Tensor\n","            hidden: torch.Tensor, [num_layers, hidden_size] or\n","                [num_layers, batch_size, hidden_size]\n","            cell: torch.Tensor, [num_layers, hidden_size] or\n","                [num_layers, batch_size, hidden_size]\n","        beam_size (optional): a non-negative integer\n","        max_len (optional): a non-negative integer\n","        teacher_forcing_ratio (optional): a float number between 0 and 1\n","\n","    Return:\n","        output: torch.Tensor, [max_len, output_size] or\n","            [max_len, batch_size, output_size]\n","        states: a tuple of two torch.Tensor\n","            hidden: torch.Tensor, [num_layers, hidden_size] or\n","                [num_layers, batch_size, hidden_size]\n","            cell: torch.Tensor, [num_layers, hidden_size] or\n","                [num_layers, batch_size, hidden_size]\n","    \"\"\"\n","    #TODO: sample until all rows have more than one EOS\n","    #TODO: forward with beam search 구현\n","    use_beam_search = beam_size != 1\n","    if use_beam_search:\n","      raise NotImplementedError()\n","    else:\n","      # input.size(0) == target length\n","      if self.training: max_len = input.size(0)\n","      \n","      is_batched = len(input.shape) == 2\n","      if is_batched:\n","        outputs = torch.zeros(max_len, input.size(1), self.output_size,\n","                             **self.factory_kwargs)\n","      else:\n","        outputs = torch.zeros(max_len, self.output_size, **self.factory_kwargs)\n","\n","      assert states is not None, \"You should give hidden states and cell \" \\\n","        \"states into the decoder\"\n","      hidden, cell = states\n","\n","      inputs = input\n","      input_shape = (1, input.size(1)) if is_batched else (1,)\n","      input = inputs[0].view(input_shape) # [1] or [1, batch_size]\n","      for i in range(1, max_len):\n","        embedded = self.embedding(input)\n","        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n","        output = self.linear(output)\n","        outputs[i] = output.view(outputs.shape[1:])\n","        if self.training and torch.randn(1) < teacher_forcing_ratio:\n","          # use teacher forcing\n","          input = inputs[i].view(input_shape)\n","        else:\n","          # do not use teacher forcing\n","          input = output.argmax(len(inputs.shape)).view(input_shape)\n","          \n","      return outputs, (hidden, cell)"],"metadata":{"id":"cCWyHQmgmv1g","executionInfo":{"status":"ok","timestamp":1650883561400,"user_tz":-540,"elapsed":10,"user":{"displayName":"김승현","userId":"01812332458933298199"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["## A Whole Seq2Seq Module"],"metadata":{"id":"GfuseFTT7Ig3"}},{"cell_type":"code","source":["class Seq2SeqNetwork(nn.Module):\n","\n","  def __init__(self, input_size, embed_size, hidden_size, output_size,\n","               num_rnn_layers, padding_index, dtype=torch.float, device='cpu'):\n","    super(Seq2SeqNetwork, self).__init__()\n","    self.input_size = input_size\n","    self.embed_size = embed_size\n","    self.hidden_size = hidden_size\n","    self.output_size = output_size\n","    self.num_rnn_layers = num_rnn_layers\n","    self.factory_kwargs = {'dtype': dtype, 'device': device}\n","\n","    self.encoder = Encoder(input_size, embed_size, hidden_size, num_rnn_layers,\n","                           padding_index, **self.factory_kwargs)\n","    self.decoder = Decoder(embed_size, hidden_size, output_size, num_rnn_layers,\n","                           padding_index, **self.factory_kwargs)\n","\n","  def forward(self, src, trg, beam_size=1, max_len=50,\n","              teacher_forcing_ratio=0.):\n","    \"\"\"Args:\n","        src: torch.Tensor, [src_len] or [src_len, batch_size]\n","        trg: torch.Tensor, [trg_len] or [trg_len, batch_size]\n","        beam_size (optional): a non-negative integer\n","        max_len (optional): a non-negative integer\n","        teacher_forcing_ratio (optional): a float number between 0 and 1\n","\n","    Return:\n","        output: torch.Tensor, [trg_len, output_size] or\n","            [trg_len, batch_size, output_size]\n","    \"\"\"\n","    _, (hidden, cell) = self.encoder(src)\n","    output, _ = self.decoder(trg, (hidden, cell), beam_size, max_len,\n","                             teacher_forcing_ratio=teacher_forcing_ratio)\n","    return output\n","\n","  def encode(self, input, states=None):\n","    \"\"\"Args:\n","        input: torch.Tensor, [seq_len] or [seq_len, batch_size]\n","        states (optional): a tuple of two torch.Tensor\n","            hidden: torch.Tensor, [num_layers, hidden_size] or\n","                [num_layers, batch_size, hidden_size]\n","            cell: torch.Tensor, [num_layers, hidden_size] or\n","                [num_layers, batch_size, hidden_size]\n","\n","    Return:\n","        output: torch.Tensor, [seq_len, hidden_size] or\n","            [trg_len, batch_size, hidden_size]\n","        states (optional): a tuple of two torch.Tensor\n","            hidden: torch.Tensor, [num_layers, hidden_size] or\n","                [num_layers, batch_size, hidden_size]\n","            cell: torch.Tensor, [num_layers, hidden_size] or\n","                [num_layers, batch_size, hidden_size]\n","    \"\"\"\n","    return self.encoder(input, states)\n","\n","  def decode(self, input, states=None, beam_size=1, max_len=50,\n","            teacher_forcing_ratio=0.):\n","    \"\"\"Args:\n","        input: torch.Tensor, [seq_len] or [seq_len, batch_size]\n","        beam_size (optional): a non-negative integer\n","        max_len (optional): a non-negative integer\n","        teacher_forcing_ratio (optional): a float number between 0 and 1\n","\n","    Return:\n","        output: torch.Tensor, [max_len, output_size] or\n","            [max_len, batch_size, output_size]\n","    \"\"\"\n","    output, _ = self.decoder(input, states, beam_size, max_len,\n","                             teacher_forcing_ratio)\n","    return output"],"metadata":{"id":"R2s6dKYxmxn1","executionInfo":{"status":"ok","timestamp":1650883561400,"user_tz":-540,"elapsed":9,"user":{"displayName":"김승현","userId":"01812332458933298199"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["# References"],"metadata":{"id":"t-0uHJW46MLB"}},{"cell_type":"markdown","source":["[1] Sequence to Sequence Learning with Neural Networks [[link]](\n","https://doi.org/10.48550/arXiv.1409.3215)\n","\n","[2] Generating Sequences With Recurrent Neural Networks [[link]](\n","https://doi.org/10.48550/arXiv.1308.0850)"],"metadata":{"id":"qZJ81DNG6TZi"}}]}